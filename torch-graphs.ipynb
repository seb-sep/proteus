{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "import torch.nn as nn\n",
    "import torch.fx as fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, out_dim, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (self.layers(x),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Model(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        # No stacktrace found for following nodes\n",
      "        layers_0 = getattr(self.layers, \"0\")(x);  x = None\n",
      "        layers_1 = getattr(self.layers, \"1\")(layers_0);  layers_0 = None\n",
      "        layers_2 = getattr(self.layers, \"2\")(layers_1);  layers_1 = None\n",
      "        layers_3 = getattr(self.layers, \"3\")(layers_2);  layers_2 = None\n",
      "        layers_4 = getattr(self.layers, \"4\")(layers_3);  layers_3 = None\n",
      "        return (layers_4,)\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class Model(torch.nn.Module):\\n    def forward(self, x):\\n        # No stacktrace found for following nodes\\n        layers_0 = getattr(self.layers, \"0\")(x);  x = None\\n        layers_1 = getattr(self.layers, \"1\")(layers_0);  layers_0 = None\\n        layers_2 = getattr(self.layers, \"2\")(layers_1);  layers_1 = None\\n        layers_3 = getattr(self.layers, \"3\")(layers_2);  layers_2 = None\\n        layers_4 = getattr(self.layers, \"4\")(layers_3);  layers_3 = None\\n        return (layers_4,)\\n        '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model(128, 32, 16)\n",
    "x = m(torch.rand((128,)))\n",
    "graph = fx.symbolic_trace(m)\n",
    "graph.print_readable()\n",
    "# for key, value in vars(graph).items(): print(key, value)\n",
    "# for node in graph.graph.nodes:\n",
    "    # for key, value in vars(node).items(): print(key, value)\n",
    "    # break\n",
    "# graph.print_readable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposed fx Graph in Aten IR:\n",
      "graph():\n",
      "    %primals_1 : [num_users=1] = placeholder[target=primals_1]\n",
      "    %primals_2 : [num_users=1] = placeholder[target=primals_2]\n",
      "    %primals_3 : [num_users=1] = placeholder[target=primals_3]\n",
      "    %primals_4 : [num_users=2] = placeholder[target=primals_4]\n",
      "    %t : [num_users=1] = call_function[target=torch.ops.aten.t.default](args = (%primals_1,), kwargs = {})\n",
      "    %mm : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%primals_4, %t), kwargs = {})\n",
      "    %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%mm,), kwargs = {})\n",
      "    %t_1 : [num_users=2] = call_function[target=torch.ops.aten.t.default](args = (%primals_2,), kwargs = {})\n",
      "    %mm_1 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%relu, %t_1), kwargs = {})\n",
      "    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%mm_1,), kwargs = {})\n",
      "    %t_2 : [num_users=2] = call_function[target=torch.ops.aten.t.default](args = (%primals_3,), kwargs = {})\n",
      "    %mm_2 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%relu_1, %t_2), kwargs = {})\n",
      "    return [mm_2, primals_4, relu, t_1, relu_1, t_2]\n",
      "{   '_args': ([mm_2, primals_4, relu, t_1, relu_1, t_2],),\n",
      "    '_erased': False,\n",
      "    '_input_nodes': {   primals_4: None,\n",
      "                        relu: None,\n",
      "                        t_1: None,\n",
      "                        relu_1: None,\n",
      "                        t_2: None,\n",
      "                        mm_2: None},\n",
      "    '_kwargs': {},\n",
      "    '_next': ,\n",
      "    '_prev': mm_2,\n",
      "    '_repr_fn': None,\n",
      "    '_sort_key': (22,),\n",
      "    'graph': <torch.fx.graph.Graph object at 0x12d82fa10>,\n",
      "    'meta': {},\n",
      "    'name': 'output',\n",
      "    'op': 'output',\n",
      "    'target': 'output',\n",
      "    'type': None,\n",
      "    'users': {}}\n"
     ]
    }
   ],
   "source": [
    "from torch._decomp import core_aten_decompositions\n",
    "import torch._dynamo\n",
    "from torch._functorch.aot_autograd import aot_module_simplified, aot_export_module\n",
    "import pprint\n",
    "\n",
    "def toy_backend(gm: fx.GraphModule, sample_inputs):\n",
    "    def my_compiler(gm, sample_inputs):\n",
    "        # <implement your compiler here>\n",
    "        print(\"Decomposed fx Graph in Aten IR:\")\n",
    "        print(gm.graph)\n",
    "        for node in gm.graph.nodes:\n",
    "            # print(node.target)\n",
    "            # print(type(node.target))\n",
    "            # if node.target == torch.ops.aten.mm.default:\n",
    "            if node.op == 'output':\n",
    "                pprint.pprint(vars(node), indent=4)\n",
    "\n",
    "        return gm\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=my_compiler\n",
    "    )\n",
    "\n",
    "m = torch.compile(m, backend=toy_backend)\n",
    "x = m(torch.rand((1, 128)))\n",
    "# x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %p_layers_0_weight : [num_users=1] = placeholder[target=p_layers_0_weight]\n",
      "    %p_layers_1_weight : [num_users=1] = placeholder[target=p_layers_1_weight]\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_layers_0_weight), kwargs = {})\n",
      "    %linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%linear, %p_layers_1_weight), kwargs = {})\n",
      "    return (linear_1,)\n"
     ]
    }
   ],
   "source": [
    "from torch.export import export \n",
    "ex_in = torch.rand((128,))\n",
    "m = Model(128, 32, 16)\n",
    "prog = export(m, args=(ex_in,))\n",
    "print(prog.graph)\n",
    "\n",
    "# torch.ops.aten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
